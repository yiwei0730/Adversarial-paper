---
title: 中研院讀書會筆記
date: 2021-06-01 22:35:00
comments: true
author: Yi-Wei
categories:
- meeting
- read note
tags:
- Adversarial
- note
---

# 中研院讀書會筆記
###### tags: `adversarial`

# 6/1 讀書會
Advancing High Fidelity Identity Swapping for Forgery Detection CVPR 2020
以前的FaceSwap 3D implementation
現在的利用深度學習來使用
https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Advancing_High_Fidelity_Identity_Swapping_for_Forgery_Detection_CVPR_2020_paper.pdf
這篇文章用了ＧＡＮ還有其另外一篇的identity encoder 組成並且中間有使用self-attention的方式，最後再用一個head net做縮小原本圖像差距

# 5/5 讀書會
THAT : two head adversarial training 
feature head and classification head  

# 4/21 讀書會
Contrastive Learning of General-Purpose Audio Representations
https://arxiv.org/pdf/2010.10915.pdf

Multi-Format Contrastive Learning of Audio Representations
https://arxiv.org/pdf/2103.06508.pdf


https://openaccess.thecvf.com/content_ICCV_2019/papers/Yoo_Photorealistic_Style_Transfer_via_Wavelet_Transforms_ICCV_2019_paper.pdf
# 3/24 讀書會
Forgery Detection
1. GAN architecture(Cycle GAN)轉換臉部之類的
2. Star GAN增加髮量等等
3. Deepfake DF, FaceSwap FS
4. Expression swap (F2F, Neural Texture)

Use Transfer learning for lack future data
Deep Distribution Transfer(DDT)
Deep One-class classification ICML 2018
Deep Semi-supervised Anomaly detection
SAD objective funtion 

Generating Adversarial yet Inconspicuous Patches with a Single Image
coarse to fine

# 3/10讀書會
Proposed variants of adversarial contrastive learning
a-S2S, b-A2A, c-A2S, d-Dual stream31
# 1/6 讀書會
Splice detection - Noise Residuals Approach
Learned Rich filter
information bottleneck learning a compressed information 


# 12/23 讀書會
exploring simple siamese representation learning
Siamese Network - collapsing to a constant problem
prevent answer : 
Constrastive learning

Swapping Assignments between Views 

(SimCLR,Moco(momentem encoder),FAIR,BYOL)
stop-gradient

# 12/9 讀書會
PGD adversarial training : 解決min-max的saddle point problem

maximum problem inside (PGD 藍色的部分)and minimum problem outside

Diversity Can Be Transferred: Output Diversification for White- and Black-box Attacks



# 11/25 讀書會 
Open-Set adversarial defense
https://github.com/rshaojimmy/ECCV2020-OSAD


排一下行程表（三個老師學生輪流）
open-set problem 讓模型知道他不知道

three loss: openset CE_loss,  reconstruction loss, transformation loss.
可以使用額外的loss function 在我的test function 以外


# 11/11 讀書會
[Characterizing Speech Adversarial Examples Using Self-Attention U-Net Enhancement](https://ieeexplore.ieee.org/document/9053288)
[Adversarial Black-Box Attacks on Automatic Speech Recognition Systems using Multi-Objective Evolutionary Optimization](https://arxiv.org/abs/1811.01312)
[Audio Adversarial Examples: Targeted Attacks on Speech-to-Text](https://arxiv.org/abs/1801.01944)
[https://arxiv.org/abs/1803.00657](Evolutionary Generative Adversarial Networks)
[Robust Audio Adversarial Example for a Physical Attack](https://www.ijcai.org/Proceedings/2019/0741.pdf)
# 10/28 讀書會 
Image Manipulation Detection 
intro 
image-forgery detection 
Face-forgery detection 
Face X-Ray(Blending Artifact)
$F^3$-Net(Frequency)

Image Manipulation Type:
splicing(拼接)
copy-move
Removal 

image-forgery(F2F)
Face-forgery(Pornography and Political)

Goal : 
把圖片放入偵測模型去偵測出anchor box被改動的區域
[ManTra-Net](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_ManTra-Net_Manipulation_Tracing_Network_for_Detection_and_Localization_of_Image_CVPR_2019_paper.pdf)
[SRM convolution, RGB-N](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Learning_Rich_Features_CVPR_2018_paper.pdf)
[Deep image Harmonization](http://vllab.ucmerced.edu/ytsai/CVPR17/cvpr17_harmonization.pdf)
[$F^3$-Net with Thinking in Frequency: Face Forgery Detection by Mining Frequency-aware Clues](https://arxiv.org/pdf/2007.09355.pdf) and [知乎](https://zhuanlan.zhihu.com/p/171799430)
[Face X-ray for More General Face Forgery Detection](https://arxiv.org/pdf/1912.13458.pdf)
[Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to Adversarial Examples](https://arxiv.org/pdf/2002.12749.pdf)
[$F^3$Net: Fusion, Feedback and Focus for Salient Object Detection](https://arxiv.org/pdf/1911.11445.pdf)
[FakeLocator: Robust Localization of GAN-Based Face Manipulations](https://arxiv.org/pdf/2001.09598.pdf)

# 9/2 讀書會
## Adversarial Examples Are Not Bugs, They Are Features
https://arxiv.org/pdf/1905.02175.pdf
Robust Feature : 人臉之類的特徵->Low frequency
Non-Robust Feature : 人類無法辨識的特徵->high frequency
![](https://i.imgur.com/xs1PCfE.png)

結論：可能需要一些人類的知識才能使模型有好的accuracy and robustness。



# 7/22 讀書會
## Speech Enhancement(SE)
目標：聲音品質和理解度，與語音辨識度
based SE system : 將noise(waveform restoration)與原本的正常的語音輸入去做feture extraction 然後將noise 去做SE後的輸出去對原本的輸出做MSE的計算
進階做法是把feature extraction 用CNN去做代替使用
(Auxiliary input:唇形，聲音震動部位，文字)
(PESQ,STOI)取代人類去計算Quality and Intelligibility
(-0.5~4.5, 0~1 )
對於音頻區間利用class weight 去關注想關注的點
## MetricGAN : Generative Adversarial Networks based Black-box Metric Scores Optimization for Speech Enhancement

Motivation : CGAN loss's adversarial loss 比較沒有幫助
解決 DEM problem ：利用 evaluation metrics(Q) 去把分類取代掉(連續取代離散)





# 7/15 Robust Physical Asarial Attack on Faster R-CNN Object Detector
前情提要 : Cybersecurity is more important
[](https://arxiv.org/pdf/1804.05810.pdf)
## physical Realizable Adversarial Attack
Digital attack has no action to internal pipline
Target Attack 選擇目標去做特定轉換(Fool Face Recoginition)

額外"Totla Variation Denoising
Mdeiian Filter"

重要方法：ShIELD 速度超級快(優點)
Stochastic Local Quantization(SLQ)

ADAGIO in speech recoginization

Unmask

Attack
Black-box in physical adversarial attack
Attack reak systems

Defense
Defend against white-box adaptive attacks
Integrity, privacy, and fairness of ML

# 7/2 中研院讀書會

#  Making an Invisibility Cloak: Real World Adversarial attacks on object Detectors
[論文網址](https://arxiv.org/pdf/1910.14667.pdf)
- 把圖片印下來貼在人的身上，讓它不會被偵測到（偽裝，CVPR(可能會很怪異有用style transfer讓他比較不會怪異 or (adversarial page?))）
## Introduction 
Problem : 
- 從電腦跳到現實去看這件事情
- 現實世界的攻擊 successful rate
- 現實世界有很多問題，光源等等 3D環境的各種因素

將誤差實體化的放在人身上不被偵測到（一個隱形披風）

Related Work
- Toxic Sign (Darts : Deceiving Automous)
- Adversarial eyeglass frames
- Adversarial patches
- hats
- detectors in a picture

Goal : 
- Universal
- Transfer

## Algorithm
- Adversarial Patch Process
1. Initialize P$\in R^{w*h*3}$ 
And a R($\theta$) function (random)
i是不同人(detector 有幾個i)， j是不同的detector
$TV(P)$ : total variation penlty
同一個patch
將圖片加上noise圖片之後丟進各種detection裡面之後，利用loss去計算他的loss

~~loss function's penality?~~

## Dataset

Coco -> Person(random choise) -> training data
YOLO的去做會比其他的更好（不知道為啥）

success rate with one image(有點奇怪哈哈)
ensemble with the realtime(?)

## Second Goal : Transferability across backbone
我好棒

Defense with this model?


他們提出的問題:
pixel 的失真
pixel 的概念


