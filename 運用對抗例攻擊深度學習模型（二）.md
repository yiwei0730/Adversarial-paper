---
title: 運用對抗例攻擊深度學習模型（二）
date: 2020-02-21 15:05:00
comments: true
categories:
- read note
tags:
- Adversarial
- note
---


# 運用對抗例攻擊深度學習模型（二）
https://medium.com/trustableai/%E9%81%8B%E7%94%A8%E5%B0%8D%E6%8A%97%E4%BE%8B%E6%94%BB%E6%93%8A%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E6%A8%A1%E5%9E%8B-%E4%BA%8C-30f80a3e7429

###### tags: `adversarial example` `robustness` `Deep learning`

[1]Kuratin et al（2016）[1]
嘗試將照片列印後，再用手機偵測，雖然有些會失敗，但$\frac{2}{3}$的對抗例依舊是成功的
[2]Sharif et al（2016)[2]
製造眼鏡框去騙過人臉辨識
[3]Open AI 的 Athalye and Sutskever（2017）[4]
幾天後提出一篇很短的報告，裡頭製造了一個不受縮放、平移、旋轉、雜訊、亮度所影響，都可以造成誤判的對抗例
[4]Evtimov et al（2017）[5]
針對美國的道路標誌，他們所製造的對抗例可以印成局部遮罩（mask），貼在路標上，從不同距離、角度拍攝後，大部份都能造成誤判


[1] Feature squeezing 是 Xu et al（2017）[7]提出的偵測與防禦手法，簡單來說就是**壓縮影像資料，讓製造對抗例時所加入的擾動被削弱**。這個作法看起來簡單又有效，論文中測試了對 color depth 壓縮與影像平滑化，證實對 JSMA 與 FGSM 所產生的對抗例都有不錯的效果，可以提昇模型的辨識正確率。這個方法屬於被動防禦，好處是不用重新訓練模型，配合論文中提到的架構做過濾的話，理論上會有一定的 false positive 比率，但是過濾後的部份辨識準確率不會下降。稍後發表的 Liang et al（2017）[8]與 Xu et al（2017）[9]也可以偵測到用 DeepFool 與 Carlini/Wagner 演算法產生的對抗例。
[2] Ensemble of specialists 則是 Abbasi and Gagné（2017）[10]稍早所提出的防禦方法，概念上是運用 Bendale and Boult（2016）[11]提到的 open set recognition，也就是**模型應該要偵測出並且拒絕掉「認不得」（unknown）的資料**，作法上則是用 ensemble of models。這樣的模型沒有針對任何對抗例做訓練，卻可以降低對於對抗例的預測信心。
不過 He et al（2017）[12]的結果可能削弱了上述兩個防禦手法的強度。He et al 所運用的威脅模型是「適應性對抗」（adaptive adversary），也就是假設攻擊方知道防禦手法為何，並且採取反制手段。在論文中他們討論了 feature squeezing 與 specialists+1 等防禦手法，如何製造出可以迴避這兩種過濾手法的對抗例。

另一方面，黑箱攻擊的手法也有進展。上次提到 Papernot et al（2017）[13]的黑箱攻擊，利用的是對抗例的可移轉性（transferability）。最近 Chen et al（2017）[14]提出了「零階最佳化」（zeroth order optimization；ZOO）、Hayes and Danezis（2017）[15]提出 adversarial service，都是不用依靠移轉就可以達成的黑箱攻擊。

